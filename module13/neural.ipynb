{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is a Neural Network?\n",
    "\n",
    "Neural networks, also known as artificial neural networks (ANN), are a set of algorithms that are modeled after the human brain. They are an advanced form of machine learning that recognizes patterns and features of input data and provides a clear quantitative output.\n",
    "\n",
    "In its simplest form, a neural network contains layers of neurons that perform individual computations. These computations are connected and weighed against one another until the neurons reach the final layer. In the final layer, the neurons return either a numerical result or an encoded categorical result.\n",
    "\n",
    "Using a neural network instead of a traditional statistical or machine learning model has many advantages. For instance, neural networks can effectively detect complex relationships within data. A neural network might be able to predict future shopping behavior based on credit card transactions, or a loan applicant's likelihood of defaulting on a loan based on their application. Additionally, neural networks have greater tolerance for messy data. They can learn to ignore noisy characteristics within a large dataset.\n",
    "\n",
    "The neural network algorithms can be too complex for humans to dissect and understand (creating a black box problem).\n",
    "Neural networks are prone to overfitting (characterizing the training data so well that the model does not effectively generalize to test data).\n",
    "\n",
    "# Perceptron, the Computational Neuron\n",
    "The perceptron model is a single neural network unit. It mimics a biological neuron by receiving input data, weighting the information, and producing a clear output.\n",
    "\n",
    "The perceptron model has four major components:\n",
    "\n",
    "Input values, typically labeled x or χ (chi, pronounced kaai, as in eye).\n",
    "\n",
    "A weight coefficient for each input value, typically labeled w or ω (omega). The weight coefficient determines the input value’s strength—that is, the impact the input value has on the network.\n",
    "\n",
    "A constant value called bias, which is added to the inputs in order to help best fit the model for a given dataset. It is typically labeled w0. So, no matter how many inputs we have, there will always be an additional value to “stir the pot.”\n",
    "\n",
    "A net summary function that aggregates all weighted inputs (note that the diagram below uses a weighted summation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a Neural Network\n",
    "\n",
    "An input layer of input values transformed by weight coefficients.\n",
    "\n",
    "A hidden layer that can contain a single neuron or multiple neurons.\n",
    "\n",
    "An output layer that reports the outcome of the value.\n",
    "\n",
    "The activation function is a mathematical function applied to the end of each neuron (that is, each individual perceptron model). This function transforms each neuron’s output into a quantitative value. The quantitative output value is then used as the input value for the next layer in the neural network model. Although activation functions can introduce both linear and nonlinear properties to a neural network, nonlinear activation functions are more common.\n",
    "\n",
    "A wide variety of activation functions exist, and each has a specific purpose. However, most neural networks will use one of the following activation functions:\n",
    "\n",
    "The linear function returns the sum of the weighted inputs without transformation.\n",
    "\n",
    "The sigmoid function transforms the neuron’s output to a range between 0 and 1, which is especially useful for predicting probabilities. A neural network that uses the sigmoid function will output a model with a characteristic S curve.\n",
    "\n",
    "The tanh function transforms the output to a range between −1 and 1, and the resulting model also forms a characteristic S curve. The tanh function is primarily used for classification between two classes.\n",
    "\n",
    "The rectified linear unit (ReLU) function returns a value from 0 to infinity. This activation function transforms any negative input to 0. It is the most commonly used activation function in neural networks due to its faster learning and simplified output. However, it is not always appropriate for simpler models.\n",
    "\n",
    "The leaky ReLU function is a “leaky” alternative to the ReLU function. This means that instead of transforming negative input values to 0, it transforms negative input values into much smaller negative values.\n",
    "\n",
    "TensorFlow is an end-to-end open-source platform for machine learning. It allows us to run our code across multiple platforms in a highly efficient way.\n",
    "\n",
    "Keras is an abstraction layer on top of TensorFlow that makes it easier to build models. We use Keras for the same reason that we use hvPlot to create charts instead of the more verbose Matplotlib library—it makes the work easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_blobs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19d63c82bb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with the dummy data\n",
    "df = pd.DataFrame(X, columns=[\"Feature 1\", \"Feature 2\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "# Plot the dummy data\n",
    "df.plot.scatter(x=\"Feature 1\", y=\"Feature 2\", c=\"Target\", colormap=\"winter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using a neural network, we must normalize, or standardize, our data. Neural networks typically perform better with all input features on the same scale. This makes it easier for the neural network to adjust the weights in the network.\n",
    "\n",
    "Developers commonly use scikit-learn's MinMaxScaler or StandardScaler functions to scale and normalize input features. For this example, we will use StandardScaler to scale the features data. We do not need to scale the target data (y), because it is already encoded as 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler instance\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network Model Using Keras\n",
    "To create a neural network, we first define an instance of the model.\n",
    "\n",
    "In this case, we will create an instance of the Sequential model and set it equal to the neuron variable. The neuron variable will store the architecture of our model, as the following code shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_inputs = 2\n",
    "number_hidden_nodes = 3\n",
    "\n",
    "neuron.add(Dense(units=number_hidden_nodes, activation=\"relu\", input_dim=number_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look more closely at the code we just wrote. Since our initial code creates both the input layer and the first hidden layer, we define the number of inputs with the input_dim parameter. We also define the number of neurons in the first hidden layer (Python calls these “hidden nodes” or “hidden neurons”) with the units parameter.\n",
    "\n",
    "We use the activation parameter to define the activation function that will process the values of the input features before they are passed to the first hidden layer. In this example, we have chosen to use the rectified linear unit (ReLU) function (Links to an external site.). It is currently the world's most-used activation function when it comes to training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 1\n",
    "\n",
    "neuron.add(Dense(units=number_classes, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we only need to supply two parameters: units and activation.\n",
    "\n",
    "units: In the units parameter, we define the number of output neurons. Because we are building a classification model, the model will output a yes or no (1 or 0) binary decision for each input data point. So, we only need one output neuron.\n",
    "activation: In the previous layer, we used a ReLU activation function to enable nonlinear relationships. Now, for our classification model’s output layer, we use the sigmoid activation function. The sigmoid function will transform the output to a range between 0 and 1. This allows the model to map the result to a probability that the input data point belongs to Class 1 (rather than Class 0). Alternatively, it would allow the model to perform a hard classification and identify each input data point as either Class 1 or Class 0. For this type of classification, the model would use a default threshold of 0.5. In other words, the model would classify any data point with an output greater than or equal to 0.5 as Class 1, and any data point with an output less than 0.5 as Class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 3)                 9         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model summary\n",
    "neuron.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary displays the structure—the output shape and number of weights—of each layer in our neuron model. The summary’s first row includes both the input layer and the hidden layer. The input layer has 2 inputs, and the hidden layer has 3 neurons. So, in total, there are 9 ((2 × 3) + 3) parameters. The summary’s second row details the output layer. It has 3 inputs and 1 output, for a total of 4 ((3 × 1) + 1) parameters. In total, the model has 13 (9 + 4) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling a Neural Network Model\n",
    "\n",
    "Now that we have defined the structure of a neural network, the next step is to compile the model. It might help to think of it this way: defining the neural network’s structure is akin to designing construction plans, and compiling the model is like building the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function\n",
    "The loss parameter specifies the loss function. When we train our neural network model on a dataset, we will pass our training dataset through the model multiple times. The loss function uses machine learning algorithms to score the performance of the model after each of these iterations. This allows us to see how the model’s performance changes over each iteration. We may determine that the model reaches maximum performance after a particular number of iterations.\n",
    "\n",
    "# The Optimization Function\n",
    "The optimizer parameter specifies the optimization function. This function shapes and molds a neural network model while the model is trained on the data, to ensure that the model performs to the best of its ability. At a glance, an optimization function reduces the model’s losses and provides the most accurate output possible.\n",
    "In this demonstration, we use the adam optimizer. This optimizer uses a gradient descent approach, which ensures that weaker classifying variables and features will not confuse the model and cause it to return less accurate results.\n",
    "\n",
    "Gradient descent is an optimization algorithm. Neural networks use it to identify the combination of function parameters that will allow the model to learn as efficiently as possible, until it has learned everything it can. When gradient descent works properly, the model learns the greatest amount in the early iterations.\n",
    "\n",
    "# The Evaluation Metrics\n",
    "Finally, the metrics parameter specifies additional metrics that assess the quality of a neural network model.\n",
    "\n",
    "There are two main evaluation metrics: model predictive accuracy and model mean squared error (MSE). We use model predictive accuracy (accuracy) for classification models, and we use MSE (mse) for regression models. For classification models, the highest possible accuracy value is 1. A higher accuracy value indicates more accurate predictions. However, for regression models, we want the MSE to reduce to zero. The closer to 0 our MSE is, the more accurate the model’s predictions are.\n",
    "\n",
    "In machine learning, an epoch is a single pass of the entire training dataset through the model. Sometimes an epoch is loosely defined as an iteration of a model.\n",
    "\n",
    "When we fit (train) a neural network model, we use the optimizer and loss functions to adjust the weights of each input during each epoch of the training cycle. In the code shown below, we fit our model using 100 epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 1s 2ms/step - loss: 0.9417 - accuracy: 0.0107\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.8932 - accuracy: 0.0013\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.8454 - accuracy: 0.0027\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.7987 - accuracy: 0.0173\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.7543 - accuracy: 0.0947\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.7113 - accuracy: 0.2800\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.6708 - accuracy: 0.4360\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.6340 - accuracy: 0.4907\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.5991 - accuracy: 0.4920\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.5672 - accuracy: 0.7707\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.5380 - accuracy: 0.9040\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.5112 - accuracy: 0.9213\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.4867 - accuracy: 0.9307\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.4643 - accuracy: 0.9453\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.4438 - accuracy: 0.9533\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.4249 - accuracy: 0.9600\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.4076 - accuracy: 0.9680\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.3917 - accuracy: 0.9733\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 739us/step - loss: 0.3770 - accuracy: 0.9773\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.3633 - accuracy: 0.9813\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.3506 - accuracy: 0.9853\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.3388 - accuracy: 0.9893\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 869us/step - loss: 0.3279 - accuracy: 0.9907\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 739us/step - loss: 0.3176 - accuracy: 0.9933\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.3079 - accuracy: 0.9933\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.9973\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.2902 - accuracy: 0.9973\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.9973\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.9987\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.2671 - accuracy: 0.9987\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2602 - accuracy: 0.9987\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.2473 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.2246 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.2145 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.2098 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.2052 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.2008 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.1965 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.1925 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1846 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.1809 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.1774 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 869us/step - loss: 0.1739 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 869us/step - loss: 0.1705 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.1672 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1640 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.1579 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1550 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.1521 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1493 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.1466 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1440 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.1414 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1389 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1365 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1341 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.1318 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1295 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.1273 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.1251 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 869us/step - loss: 0.1230 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.1210 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 958us/step - loss: 0.1190 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1170 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1151 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.1132 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.1114 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.1096 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.1078 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 914us/step - loss: 0.1061 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.1044 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1028 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.1012 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0996 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 827us/step - loss: 0.0981 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0965 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0936 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0922 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.0908 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 827us/step - loss: 0.0895 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.0881 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0868 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.0855 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.0843 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.0831 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.0819 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.0807 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 826us/step - loss: 0.0795 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 783us/step - loss: 0.0784 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.0772 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.0761 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 913us/step - loss: 0.0750 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model = neuron.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Loss and Accuracy\n",
    "After the training cycle ends, we can visually evaluate the model by plotting the loss function and the accuracy across all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19d6c43e9a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with the history dictionary\n",
    "df = pd.DataFrame(model.history, index=range(1, len(model.history[\"loss\"]) + 1))\n",
    "\n",
    "# Plot the loss\n",
    "df.plot(y=\"loss\")\n",
    "\n",
    "# Plot the accuracy\n",
    "df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model’s Performance\n",
    "Now that our model has been properly trained, we can evaluate its performance using the test data. Testing a neural network model in TensorFlow is similar to testing a machine learning model in scikit-learn.\n",
    "\n",
    "For this demonstration, we'll use the evaluate function. Then, we’ll print the testing loss and accuracy values, as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 0.0700 - accuracy: 1.0000 - 118ms/epoch - 15ms/step\n",
      "Loss: 0.06997469067573547, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using testing data\n",
    "model_loss, model_accuracy = neuron.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions with a Neural Network Model\n",
    "Using our neural network model, we can use the predict function to generate predictions on new data by supplying the function with the data and a threshold of 0.5. Any value under 0.5 is classified as 0 and anything over 0.5 is classified as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create 10 new samples of dummy data\n",
    "new_X, new_y = make_blobs(n_samples=10, centers=2, n_features=2, random_state=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = (neuron.predict(new_X) > 0.5).astype(\"int32\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictions  actual\n",
       "0            1       1\n",
       "1            1       1\n",
       "2            1       1\n",
       "3            1       1\n",
       "4            0       0\n",
       "5            0       0\n",
       "6            0       0\n",
       "7            1       1\n",
       "8            1       0\n",
       "9            0       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to compare the predictions with the actual values\n",
    "results = pd.DataFrame({\"predictions\": predictions.ravel(), \"actual\": new_y})\n",
    "\n",
    "# Display sample data\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Models\n",
    "Deep learning models are neural networks with multiple hidden layers. Unlike most other machine learning algorithms, including simple neural networks like the ones we created in the previous lesson, these models can discover nonlinear relationships among data. Therefore, deep learning models often perform better than other models when analyzing complex or unstructured data, such as images, text, or voice.\n",
    "\n",
    "The number of hidden layers within a neural network model determines whether the model is considered deep learning. Precise definitions vary by source. In this course, we’ll classify any neural network with more than one hidden layer as “deep.” The following image demonstrates the difference between a neural network and a deep neural network (also called a deep neural net):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./wine_quality.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features (X) and target (y) sets\n",
    "X = df.drop(columns=[\"quality\"]).values\n",
    "y = df[\"quality\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Create the scaler instance\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the features data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a deep neural net with Keras, we will use a process similar to the one we used to define a simple neural network. But, this time, we will add an additional hidden layer. In this demonstration, the second hidden layer will contain fewer neurons than the first. This is typically how deep neural nets are constructed: the number of neurons on each successive layer is equal to or less than the number of neurons on the previous layer, with the output layer containing the fewest neurons. As we did for our previous neural network, we will choose an activation function for the first layer. This time, we will also use this same activation function for our second hidden layer. Often, developers experiment with many potential architectures in an effort to minimize the loss metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net with two hidden layers\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 4\n",
    "\n",
    "# Create a sequential neural network model\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn.add(Dense(units=1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 37.3242 - mse: 37.3242\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 33.1446 - mse: 33.1446\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 29.3932 - mse: 29.3932\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 25.4745 - mse: 25.4745\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 21.2824 - mse: 21.2824\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 16.9616 - mse: 16.9616\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 12.7051 - mse: 12.7051\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 8.8856 - mse: 8.8856\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 5.9419 - mse: 5.9419\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 4.3571 - mse: 4.3571\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 972us/step - loss: 3.6515 - mse: 3.6515\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 3.2569 - mse: 3.2569\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 2.9389 - mse: 2.9389\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 2.6769 - mse: 2.6769\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 974us/step - loss: 2.4573 - mse: 2.4573\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 2.2702 - mse: 2.2702\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 2.1037 - mse: 2.1037\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.9600 - mse: 1.9600\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 1.8284 - mse: 1.8284\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 1.7065 - mse: 1.7065\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 1.5994 - mse: 1.5994\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 1.4992 - mse: 1.4992\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 1.4109 - mse: 1.4109\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 1.3268 - mse: 1.3268\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.2525 - mse: 1.2525\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 1.1815 - mse: 1.1815\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 757us/step - loss: 1.1196 - mse: 1.1196\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.0635 - mse: 1.0635\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.0108 - mse: 1.0108\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.9633 - mse: 0.9633\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.9197 - mse: 0.9197\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.8817 - mse: 0.8817\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.8430 - mse: 0.8430\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.8092 - mse: 0.8092\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.7768 - mse: 0.7768\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.7475 - mse: 0.7475\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.7217 - mse: 0.7217\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.6995 - mse: 0.6995\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6802 - mse: 0.6802\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.6618 - mse: 0.6618\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6444 - mse: 0.6444\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.6277 - mse: 0.6277\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.6165 - mse: 0.6165\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.6022 - mse: 0.6022\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.5901 - mse: 0.5901\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5802 - mse: 0.5802\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.5718 - mse: 0.5718\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5625 - mse: 0.5625\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.5541 - mse: 0.5541\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5474 - mse: 0.5474\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.5425 - mse: 0.5425\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5347 - mse: 0.5347\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5290 - mse: 0.5290\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5240 - mse: 0.5240\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.5177 - mse: 0.5177\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5143 - mse: 0.5143\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.5092 - mse: 0.5092\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.5069 - mse: 0.5069\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5024 - mse: 0.5024\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4987 - mse: 0.4987\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4966 - mse: 0.4966\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4911 - mse: 0.4911\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4895 - mse: 0.4895\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4866 - mse: 0.4866\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4827 - mse: 0.4827\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.4796 - mse: 0.4796\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4778 - mse: 0.4778\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4739 - mse: 0.4739\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.4732 - mse: 0.4732\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4705 - mse: 0.4705\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4690 - mse: 0.4690\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4668 - mse: 0.4668\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4645 - mse: 0.4645\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4618 - mse: 0.4618\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4595 - mse: 0.4595\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4596 - mse: 0.4596\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4562 - mse: 0.4562\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4533 - mse: 0.4533\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4525 - mse: 0.4525\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4510 - mse: 0.4510\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.4496 - mse: 0.4496\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4491 - mse: 0.4491\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4468 - mse: 0.4468\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.4471 - mse: 0.4471\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.4441 - mse: 0.4441\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.4456 - mse: 0.4456\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4407 - mse: 0.4407\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4397 - mse: 0.4397\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4372 - mse: 0.4372\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4362 - mse: 0.4362\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.4360 - mse: 0.4360\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4352 - mse: 0.4352\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4336 - mse: 0.4336\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4329 - mse: 0.4329\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4326 - mse: 0.4326\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.4300 - mse: 0.4300\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4288 - mse: 0.4288\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4289 - mse: 0.4289\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4280 - mse: 0.4280\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 35.3602 - mse: 35.3602\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 26.6517 - mse: 26.6517\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 18.5134 - mse: 18.5134\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 10.5886 - mse: 10.5886\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 972us/step - loss: 5.2255 - mse: 5.2255\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 3.2704 - mse: 3.2704\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 2.5803 - mse: 2.5803\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 2.1904 - mse: 2.1904\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 1.9319 - mse: 1.9319\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.7407 - mse: 1.7407\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 1.5877 - mse: 1.5877\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 1.4784 - mse: 1.4784\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 947us/step - loss: 1.3849 - mse: 1.3849\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 1.3085 - mse: 1.3085\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2416 - mse: 1.2416\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 1.1845 - mse: 1.1845\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 1.1274 - mse: 1.1274\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 999us/step - loss: 1.0739 - mse: 1.0739\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0234 - mse: 1.0234\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9802 - mse: 0.9802\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9358 - mse: 0.9358\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8954 - mse: 0.8954\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.8541 - mse: 0.8541\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.8199 - mse: 0.8199\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.7836 - mse: 0.7836\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.7472 - mse: 0.7472\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.7155 - mse: 0.7155\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6898 - mse: 0.6898\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.6626 - mse: 0.6626\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6414 - mse: 0.6414\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.6197 - mse: 0.6197\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6000 - mse: 0.6000\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5851 - mse: 0.5851\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.5694 - mse: 0.5694\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5555 - mse: 0.5555\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.5436 - mse: 0.5436\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5320 - mse: 0.5320\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5216 - mse: 0.5216\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5076 - mse: 0.5076\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5015 - mse: 0.5015\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.4893 - mse: 0.4893\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4809 - mse: 0.4809\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4732 - mse: 0.4732\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.4673 - mse: 0.4673\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4582 - mse: 0.4582\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4527 - mse: 0.4527\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4464 - mse: 0.4464\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4409 - mse: 0.4409\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4381 - mse: 0.4381\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4333 - mse: 0.4333\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4267 - mse: 0.4267\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4214 - mse: 0.4214\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.4197 - mse: 0.4197\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4129 - mse: 0.4129\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4095 - mse: 0.4095\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4091 - mse: 0.4091\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4028 - mse: 0.4028\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4016 - mse: 0.4016\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3978 - mse: 0.3978\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3915 - mse: 0.3915\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3871 - mse: 0.3871\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3906 - mse: 0.3906\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3865 - mse: 0.3865\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3847 - mse: 0.3847\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3813 - mse: 0.3813\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3781 - mse: 0.3781\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3766 - mse: 0.3766\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3751 - mse: 0.3751\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3776 - mse: 0.3776\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3740 - mse: 0.3740\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3723 - mse: 0.3723\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.3700 - mse: 0.3700\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3676 - mse: 0.3676\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3714 - mse: 0.3714\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3681 - mse: 0.3681\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3633 - mse: 0.3633\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3669 - mse: 0.3669\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3635 - mse: 0.3635\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3594 - mse: 0.3594\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3568 - mse: 0.3568\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3556 - mse: 0.3556\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3575 - mse: 0.3575\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3572 - mse: 0.3572\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3571 - mse: 0.3571\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3560 - mse: 0.3560\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3531 - mse: 0.3531\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3530 - mse: 0.3530\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3503 - mse: 0.3503\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3465 - mse: 0.3465\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3465 - mse: 0.3465\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3451 - mse: 0.3451\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3421 - mse: 0.3421\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3410 - mse: 0.3410\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3434 - mse: 0.3434\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3411 - mse: 0.3411\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3377 - mse: 0.3377\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3389 - mse: 0.3389\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3379 - mse: 0.3379\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3354 - mse: 0.3354\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3340 - mse: 0.3340\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net with two hidden layers\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer1 = 22\n",
    "hidden_nodes_layer2 = 11\n",
    "\n",
    "# Create a sequential neural network model\n",
    "nn_1 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_1.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_1.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn_1.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_1.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_model_1 = nn_1.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 30.5746 - mse: 30.5746\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 24.5571 - mse: 24.5571\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 13.0231 - mse: 13.0231\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 3.0257 - mse: 3.0257\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 2.1896 - mse: 2.1896\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 1.8573 - mse: 1.8573\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 1.6437 - mse: 1.6437\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.4839 - mse: 1.4839\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.3704 - mse: 1.3704\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2671 - mse: 1.2671\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1735 - mse: 1.1735\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0956 - mse: 1.0956\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 1.0331 - mse: 1.0331\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.9637 - mse: 0.9637\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9146 - mse: 0.9146\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8547 - mse: 0.8547\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8105 - mse: 0.8105\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.7746 - mse: 0.7746\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.7356 - mse: 0.7356\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.6994 - mse: 0.6994\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.6709 - mse: 0.6709\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6418 - mse: 0.6418\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6175 - mse: 0.6175\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5948 - mse: 0.5948\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.5811 - mse: 0.5811\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.5592 - mse: 0.5592\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.5424 - mse: 0.5424\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5302 - mse: 0.5302\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5145 - mse: 0.5145\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5050 - mse: 0.5050\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4925 - mse: 0.4925\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4828 - mse: 0.4828\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4723 - mse: 0.4723\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4643 - mse: 0.4643\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4591 - mse: 0.4591\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4483 - mse: 0.4483\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4464 - mse: 0.4464\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4360 - mse: 0.4360\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4311 - mse: 0.4311\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4250 - mse: 0.4250\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.4257 - mse: 0.4257\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4200 - mse: 0.4200\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.4148 - mse: 0.4148\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4085 - mse: 0.4085\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4035 - mse: 0.4035\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4028 - mse: 0.4028\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3994 - mse: 0.3994\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3994 - mse: 0.3994\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3948 - mse: 0.3948\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3911 - mse: 0.3911\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3908 - mse: 0.3908\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3858 - mse: 0.3858\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3833 - mse: 0.3833\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3790 - mse: 0.3790\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3777 - mse: 0.3777\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3739 - mse: 0.3739\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3800 - mse: 0.3800\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3778 - mse: 0.3778\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3716 - mse: 0.3716\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3722 - mse: 0.3722\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3653 - mse: 0.3653\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3643 - mse: 0.3643\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3599 - mse: 0.3599\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3598 - mse: 0.3598\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3656 - mse: 0.3656\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3582 - mse: 0.3582\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3575 - mse: 0.3575\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3553 - mse: 0.3553\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3552 - mse: 0.3552\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3552 - mse: 0.3552\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3527 - mse: 0.3527\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3492 - mse: 0.3492\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3462 - mse: 0.3462\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3488 - mse: 0.3488\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3433 - mse: 0.3433\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.3428 - mse: 0.3428\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3390 - mse: 0.3390\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3413 - mse: 0.3413\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3540 - mse: 0.3540\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3416 - mse: 0.3416\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3340 - mse: 0.3340\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3440 - mse: 0.3440\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3320 - mse: 0.3320\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.3294 - mse: 0.3294\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3320 - mse: 0.3320\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3330 - mse: 0.3330\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3288 - mse: 0.3288\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3270 - mse: 0.3270\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3218 - mse: 0.3218\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3274 - mse: 0.3274\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3241 - mse: 0.3241\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3220 - mse: 0.3220\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3245 - mse: 0.3245\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3223 - mse: 0.3223\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3196 - mse: 0.3196\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3207 - mse: 0.3207\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3194 - mse: 0.3194\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3163 - mse: 0.3163\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3136 - mse: 0.3136\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net with two hidden layers\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer1 = 22\n",
    "hidden_nodes_layer2 = 11\n",
    "hidden_nodes_layer3 = 8\n",
    "\n",
    "# Create a sequential neural network model\n",
    "nn_2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the third hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn_2.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_model_2 = nn_2.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 0s - loss: 0.4313 - mse: 0.4313 - 84ms/epoch - 6ms/step\n",
      "13/13 - 0s - loss: 0.4402 - mse: 0.4402 - 100ms/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model 1 using testing data\n",
    "model1_loss, model1_mse = nn_1.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Evaluate Model 2 using testing data\n",
    "model2_loss, model2_mse = nn_2.evaluate(X_test_scaled, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the preceding image, Model 1’s MSE is 0.4594, and Model 2’s MSE is 0.4584.\n",
    "\n",
    "These results tell us that the difference between the two models’ mse metrics is quite small. So, in this case, we can use the simpler model with two hidden layers to make predictions.\n",
    "\n",
    "Adding layers does not always guarantee better model performance. Depending on the input data’s complexity, adding more hidden layers sometimes just increases the chance of overfitting the training data. Unfortunately, no easy solution or rule of thumb exists to identify how many layers will maximize performance for a given model. Trial and error is the only way to determine how “deep” a deep learning model should be. You must train and evaluate a model with deeper and deeper layers, until the model no longer demonstrates noticeable improvements over the same number of epochs.\n",
    "\n",
    "Next, we’ll put all of your new skills into practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optimizing Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Current_Loan_Amount</th>\n",
       "      <th>Term</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Years_in_current_job</th>\n",
       "      <th>Home_Ownership</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Monthly_Debt</th>\n",
       "      <th>Years_of_Credit_History</th>\n",
       "      <th>Months_since_last_delinquent</th>\n",
       "      <th>Number_of_Open_Accounts</th>\n",
       "      <th>Number_of_Credit_Problems</th>\n",
       "      <th>Current_Credit_Balance</th>\n",
       "      <th>Maximum_Open_Credit</th>\n",
       "      <th>Bankruptcies</th>\n",
       "      <th>Tax_Liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2231892.0</td>\n",
       "      <td>8_years</td>\n",
       "      <td>Own_Home</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>29200.53</td>\n",
       "      <td>14.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>297996</td>\n",
       "      <td>750090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>217646</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1184194.0</td>\n",
       "      <td>&lt;_1_year</td>\n",
       "      <td>Home_Mortgage</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>10855.08</td>\n",
       "      <td>19.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>122170</td>\n",
       "      <td>272052.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>548746</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>678.0</td>\n",
       "      <td>2559110.0</td>\n",
       "      <td>2_years</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>18660.28</td>\n",
       "      <td>22.6</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>437171</td>\n",
       "      <td>555038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>728.0</td>\n",
       "      <td>714628.0</td>\n",
       "      <td>3_years</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>11851.06</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>203965</td>\n",
       "      <td>289784.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fully_Paid</td>\n",
       "      <td>99999999</td>\n",
       "      <td>Short_Term</td>\n",
       "      <td>740.0</td>\n",
       "      <td>776188.0</td>\n",
       "      <td>&lt;_1_year</td>\n",
       "      <td>Own_Home</td>\n",
       "      <td>Debt_Consolidation</td>\n",
       "      <td>11578.22</td>\n",
       "      <td>8.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>134083</td>\n",
       "      <td>220220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Loan_Status  Current_Loan_Amount        Term  Credit_Score  Annual_Income  \\\n",
       "0  Fully_Paid             99999999  Short_Term         741.0      2231892.0   \n",
       "1  Fully_Paid               217646  Short_Term         730.0      1184194.0   \n",
       "2  Fully_Paid               548746  Short_Term         678.0      2559110.0   \n",
       "3  Fully_Paid             99999999  Short_Term         728.0       714628.0   \n",
       "4  Fully_Paid             99999999  Short_Term         740.0       776188.0   \n",
       "\n",
       "  Years_in_current_job Home_Ownership             Purpose  Monthly_Debt  \\\n",
       "0              8_years       Own_Home  Debt_Consolidation      29200.53   \n",
       "1             <_1_year  Home_Mortgage  Debt_Consolidation      10855.08   \n",
       "2              2_years           Rent  Debt_Consolidation      18660.28   \n",
       "3              3_years           Rent  Debt_Consolidation      11851.06   \n",
       "4             <_1_year       Own_Home  Debt_Consolidation      11578.22   \n",
       "\n",
       "   Years_of_Credit_History  Months_since_last_delinquent  \\\n",
       "0                     14.9                          29.0   \n",
       "1                     19.6                          10.0   \n",
       "2                     22.6                          33.0   \n",
       "3                     16.0                          76.0   \n",
       "4                      8.5                          25.0   \n",
       "\n",
       "   Number_of_Open_Accounts  Number_of_Credit_Problems  Current_Credit_Balance  \\\n",
       "0                       18                          1                  297996   \n",
       "1                       13                          1                  122170   \n",
       "2                        4                          0                  437171   \n",
       "3                       16                          0                  203965   \n",
       "4                        6                          0                  134083   \n",
       "\n",
       "   Maximum_Open_Credit  Bankruptcies  Tax_Liens  \n",
       "0             750090.0           0.0        0.0  \n",
       "1             272052.0           1.0        0.0  \n",
       "2             555038.0           0.0        0.0  \n",
       "3             289784.0           0.0        0.0  \n",
       "4             220220.0           0.0        0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file from the Resources folder into a Pandas DataFrame\n",
    "df = pd.read_csv(\n",
    "    Path(\"./loan_status.csv\")\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loan_Status', 'Term', 'Years_in_current_job', 'Home_Ownership', 'Purpose']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.dtypes[df.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the columns with categorical variables\n",
    "categorical_variables = [\"Loan_Status\", \"Term\", \"Years_in_current_job\", \"Home_Ownership\", \"Purpose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fit_transform function from the OneHotEncoder to encode the data\n",
    "encoded_data = enc.fit_transform(df[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_Status_Fully_Paid</th>\n",
       "      <th>Loan_Status_Not_Paid</th>\n",
       "      <th>Term_Long_Term</th>\n",
       "      <th>Term_Short_Term</th>\n",
       "      <th>Years_in_current_job_10+_years</th>\n",
       "      <th>Years_in_current_job_1_year</th>\n",
       "      <th>Years_in_current_job_2_years</th>\n",
       "      <th>Years_in_current_job_3_years</th>\n",
       "      <th>Years_in_current_job_4_years</th>\n",
       "      <th>Years_in_current_job_5_years</th>\n",
       "      <th>...</th>\n",
       "      <th>Home_Ownership_Home_Mortgage</th>\n",
       "      <th>Home_Ownership_Own_Home</th>\n",
       "      <th>Home_Ownership_Rent</th>\n",
       "      <th>Purpose_Business_Loan</th>\n",
       "      <th>Purpose_Buy_House</th>\n",
       "      <th>Purpose_Buy_a_Car</th>\n",
       "      <th>Purpose_Debt_Consolidation</th>\n",
       "      <th>Purpose_Home_Improvements</th>\n",
       "      <th>Purpose_Medical_Bills</th>\n",
       "      <th>Purpose_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Loan_Status_Fully_Paid  Loan_Status_Not_Paid  Term_Long_Term  \\\n",
       "0                     1.0                   0.0             0.0   \n",
       "1                     1.0                   0.0             0.0   \n",
       "2                     1.0                   0.0             0.0   \n",
       "3                     1.0                   0.0             0.0   \n",
       "4                     1.0                   0.0             0.0   \n",
       "\n",
       "   Term_Short_Term  Years_in_current_job_10+_years  \\\n",
       "0              1.0                             0.0   \n",
       "1              1.0                             0.0   \n",
       "2              1.0                             0.0   \n",
       "3              1.0                             0.0   \n",
       "4              1.0                             0.0   \n",
       "\n",
       "   Years_in_current_job_1_year  Years_in_current_job_2_years  \\\n",
       "0                          0.0                           0.0   \n",
       "1                          0.0                           0.0   \n",
       "2                          0.0                           1.0   \n",
       "3                          0.0                           0.0   \n",
       "4                          0.0                           0.0   \n",
       "\n",
       "   Years_in_current_job_3_years  Years_in_current_job_4_years  \\\n",
       "0                           0.0                           0.0   \n",
       "1                           0.0                           0.0   \n",
       "2                           0.0                           0.0   \n",
       "3                           1.0                           0.0   \n",
       "4                           0.0                           0.0   \n",
       "\n",
       "   Years_in_current_job_5_years  ...  Home_Ownership_Home_Mortgage  \\\n",
       "0                           0.0  ...                           0.0   \n",
       "1                           0.0  ...                           1.0   \n",
       "2                           0.0  ...                           0.0   \n",
       "3                           0.0  ...                           0.0   \n",
       "4                           0.0  ...                           0.0   \n",
       "\n",
       "   Home_Ownership_Own_Home  Home_Ownership_Rent  Purpose_Business_Loan  \\\n",
       "0                      1.0                  0.0                    0.0   \n",
       "1                      0.0                  0.0                    0.0   \n",
       "2                      0.0                  1.0                    0.0   \n",
       "3                      0.0                  1.0                    0.0   \n",
       "4                      1.0                  0.0                    0.0   \n",
       "\n",
       "   Purpose_Buy_House  Purpose_Buy_a_Car  Purpose_Debt_Consolidation  \\\n",
       "0                0.0                0.0                         1.0   \n",
       "1                0.0                0.0                         1.0   \n",
       "2                0.0                0.0                         1.0   \n",
       "3                0.0                0.0                         1.0   \n",
       "4                0.0                0.0                         1.0   \n",
       "\n",
       "   Purpose_Home_Improvements  Purpose_Medical_Bills  Purpose_Other  \n",
       "0                        0.0                    0.0            0.0  \n",
       "1                        0.0                    0.0            0.0  \n",
       "2                        0.0                    0.0            0.0  \n",
       "3                        0.0                    0.0            0.0  \n",
       "4                        0.0                    0.0            0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables)\n",
    ")\n",
    "\n",
    "# Display sample data\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Layers\n",
    "Deep neural networks are robust tools, but sometimes we don’t need a deep learning model to solve a business problem. For example, when dealing with data that is linearly separable, you do not typically need any hidden layers.\n",
    "\n",
    "Recall that linearly separable data can be separated by a straight line when it’s plotted in two dimensions. For example, the following image depicts a linearly separable dataset that represents approved and not-approved credit card applications.\n",
    "\n",
    "# Number of Nodes in Each Hidden Layer\n",
    "The number of neurons that you include in each hidden layer can impact a deep learning model’s final output. So, this is an important decision. Using too few neurons could lead to underfitting the model—meaning you will not achieve the model’s best possible performance. On the other hand, including too many neurons could result in overfitting—meaning the model may not generalize well to other datasets.\n",
    "\n",
    "Developers use a few methods to determine the optimal number of neurons for a hidden layer:\n",
    "\n",
    "Find the mean of the number of input features and the number of neurons in the output layer ((number of input features + number of neurons in output layer) / 2). Use a number close to this mean for the number of neurons in the first hidden layer. Repeat this pattern for subsequent hidden layers ((number of neurons in the prior hidden layer + number of neurons in output layer) / 2). This rule normally works well for the first approximation.\n",
    "\n",
    "The total number of neurons across all hidden layers should be ⅔ the size of the input layer (size of input layer = number of features), plus the size of the output layer (size of output layer = number of neurons on the output layer).\n",
    "\n",
    "Alternatively, the total number of neurons across all hidden layers should be less than twice the size of the number of features in the input layer.\n",
    "\n",
    "These rules provide a starting point for designing a neural network. Ultimately, however, we can only discover the best architectures for our neural network models by trial-and-error testing.\n",
    "\n",
    "# Activation and Loss Functions\n",
    "Selecting the best activation and loss functions is, once again, part experience and part trial-and-error testing. As you gain more experience using neural networks, and machine learning in general, you will develop preferences for certain functions in certain use cases. In the meantime, you can use the following recommendations as starting points:\n",
    "\n",
    "For the activation function on a hidden layer, use relu.\n",
    "\n",
    "For the activation function on an output layer, use sigmoid for binary classification, softmax for multi-class classification, and linear for regression.\n",
    "\n",
    "Choose the loss function based on the type of problem you’re solving, too. For binary classification, use binary_crossentropy. For multi-class classification, use categorical_crossentropy if you encode the variables using OneHotEncoder. Or, use sparce_categorical_crossentropy if the labels are integers. Finally, use mse for regression.\n",
    "\n",
    "# Number of Epochs\n",
    "The number of epochs that we run can have an important effect on the model. Achieving the optimal number of epics helps improve the model's evaluation metrics. Usually, we vary the number of epochs as we test a model, seeking to minimize the model's loss value.\n",
    "\n",
    "A good rule of thumb is to start training a model with 20 epochs. Then, we can plot the model’s loss value and evaluation metric over the course of those 20 epochs. With these plots, we can verify whether the model’s training loss decreases over the epochs, and whether its accuracy (for classification) increases, or its mean squared error (for regression) moves toward zero. We can then continue training the model, increasing the number of epochs by 20 on each trial. After each new trial, we’ll again verify that the model’s loss and evaluation metric move in the directions we want.\n",
    "\n",
    "# Saving and Loading a Neural Network Model\n",
    "Neural networks, especially complex neural networks, may demand significant computational resources, including CPU memory and activity on the allotted server. Training a complex neural network on a medium or large dataset can take hours (or even days)! For simple modeling problems like the ones covered in this module, we can train a model in the same notebook where we analyze our data. However, for more formal applications of neural networks and deep learning models, data scientists cannot afford the time or resources to build and train a model each time they analyze data. In these cases, they must store and access trained models outside of the training environment.\n",
    "\n",
    "Data scientists publish trained models in scientific papers, deploy them in software, share them on GitHub, and even pass them along to colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 25.0471 - mse: 25.0471\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 20.1252 - mse: 20.1252\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 945us/step - loss: 14.7726 - mse: 14.7726\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 9.9991 - mse: 9.9991\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 6.7168 - mse: 6.7168\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 4.8066 - mse: 4.8066\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 3.7587 - mse: 3.7587\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 3.1245 - mse: 3.1245\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 2.6980 - mse: 2.6980\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 2.4152 - mse: 2.4152\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 2.1917 - mse: 2.1917\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 2.0101 - mse: 2.0101\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 1.8635 - mse: 1.8635\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.7276 - mse: 1.7276\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 1.6164 - mse: 1.6164\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 1.5191 - mse: 1.5191\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.4346 - mse: 1.4346\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 1.3563 - mse: 1.3563\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 1.2879 - mse: 1.2879\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.2264 - mse: 1.2264\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 1.1695 - mse: 1.1695\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 1.1242 - mse: 1.1242\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.0718 - mse: 1.0718\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 1.0311 - mse: 1.0311\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.9931 - mse: 0.9931\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.9596 - mse: 0.9596\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.9271 - mse: 0.9271\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.8989 - mse: 0.8989\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.8725 - mse: 0.8725\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.8513 - mse: 0.8513\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.8261 - mse: 0.8261\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.8076 - mse: 0.8076\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.7845 - mse: 0.7845\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.7667 - mse: 0.7667\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.7489 - mse: 0.7489\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.7329 - mse: 0.7329\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.7174 - mse: 0.7174\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.7024 - mse: 0.7024\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.6914 - mse: 0.6914\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.6769 - mse: 0.6769\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.6635 - mse: 0.6635\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.6533 - mse: 0.6533\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.6434 - mse: 0.6434\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6310 - mse: 0.6310\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.6203 - mse: 0.6203\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.6091 - mse: 0.6091\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.5987 - mse: 0.5987\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.5898 - mse: 0.5898\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5816 - mse: 0.5816\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.5714 - mse: 0.5714\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5632 - mse: 0.5632\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5552 - mse: 0.5552\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5465 - mse: 0.5465\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.5408 - mse: 0.5408\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.5347 - mse: 0.5347\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.5265 - mse: 0.5265\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.5197 - mse: 0.5197\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.5130 - mse: 0.5130\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5066 - mse: 0.5066\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5011 - mse: 0.5011\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.4946 - mse: 0.4946\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4882 - mse: 0.4882\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4824 - mse: 0.4824\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4776 - mse: 0.4776\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4728 - mse: 0.4728\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4694 - mse: 0.4694\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4651 - mse: 0.4651\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4623 - mse: 0.4623\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4562 - mse: 0.4562\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.4525 - mse: 0.4525\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.4483 - mse: 0.4483\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4475 - mse: 0.4475\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4438 - mse: 0.4438\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4408 - mse: 0.4408\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4389 - mse: 0.4389\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4338 - mse: 0.4338\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4312 - mse: 0.4312\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4306 - mse: 0.4306\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4258 - mse: 0.4258\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4240 - mse: 0.4240\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4217 - mse: 0.4217\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4202 - mse: 0.4202\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.4211 - mse: 0.4211\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.4183 - mse: 0.4183\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4177 - mse: 0.4177\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.4165 - mse: 0.4165\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4123 - mse: 0.4123\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4101 - mse: 0.4101\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4115 - mse: 0.4115\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4093 - mse: 0.4093\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4122 - mse: 0.4122\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4056 - mse: 0.4056\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.4051 - mse: 0.4051\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4043 - mse: 0.4043\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4025 - mse: 0.4025\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4022 - mse: 0.4022\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.4015 - mse: 0.4015\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3986 - mse: 0.3986\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3997 - mse: 0.3997\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4011 - mse: 0.4011\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net with two hidden layers\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 6\n",
    "# Create a sequential neural network model\n",
    "nn_1 = Sequential()\n",
    "\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_1.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_1.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn_1.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile model\n",
    "nn_1.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_model_1 = nn_1.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
